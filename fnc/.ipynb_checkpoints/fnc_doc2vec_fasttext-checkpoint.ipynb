{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import score as s\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bodies = pd.read_csv('data/train_bodies.csv')\n",
    "train_stances = pd.read_csv('data/train_stances.csv')\n",
    "train = train_stances.merge(train_bodies, on='Body ID')\n",
    "\n",
    "test_bodies = pd.read_csv('data/competition_test_bodies.csv')\n",
    "test_stances = pd.read_csv('data/competition_test_stances.csv')\n",
    "test = test_stances.merge(test_bodies, on='Body ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It strange, but when I add `articleBody` that way (all train dataset, where one articleBody included more than once), fasttext train more accurate representations and final score is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bd = train['articleBody']\n",
    "train_headline = train['Headline']\n",
    "test_bd = test['articleBody']\n",
    "test_headline = test['Headline']\n",
    "all_texts = np.concatenate((train_bd, train_headline, test_bd, test_headline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext skipgram requires text file as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/texts.txt', 'w') as f:\n",
    "    f.writelines(all_texts)\n",
    "    \n",
    "word2vec = fasttext.skipgram('data/texts.txt', 'model/fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec(tokens, word2vec):\n",
    "    word_vecs = np.array([word2vec[token] for token in tokens])\n",
    "    vec = normalize(word_vecs).mean(axis=0)\n",
    "    assert len(vec) == 100\n",
    "    return vec\n",
    "\n",
    "def get_tokens(text, stopwords, tokenizer):\n",
    "    return list(filter(lambda x: x not in stopwords, tokenizer(text)))\n",
    "\n",
    "def get_vectors(df, word2vec):\n",
    "    sw = stopwords.words('english')\n",
    "    headline_vecs = np.array(list(map(lambda text: doc2vec(get_tokens(text, sw, word_tokenize), word2vec), df['Headline'])))\n",
    "    body_vecs = np.array(list(map(lambda text: doc2vec(get_tokens(text, sw, word_tokenize), word2vec), df['articleBody'])))\n",
    "    return headline_vecs, body_vecs\n",
    "\n",
    "train_headline_vectors, train_body_vectors = get_vectors(train, word2vec)\n",
    "test_headline_vectors, test_body_vectors = get_vectors(test, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cosine = np.array(list(map(lambda a: cosine_similarity(a[0].reshape(1, -1),a[1].reshape(1, -1))[0,0], zip(train_headline_vectors, train_body_vectors))))\n",
    "test_cosine = np.array(list(map(lambda a: cosine_similarity(a[0].reshape(1, -1),a[1].reshape(1, -1))[0,0], zip(test_headline_vectors, test_body_vectors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label(n):\n",
    "    if n == \"agree\": return 0.0\n",
    "    if n == \"discuss\": return 1.0\n",
    "    if n == \"disagree\": return 2.0\n",
    "    if n == \"unrelated\": return 3.0\n",
    "    \n",
    "def unlabel(n):\n",
    "    if n == 0.0: return \"agree\"\n",
    "    if n == 1.0: return \"discuss\"\n",
    "    if n == 2.0: return \"disagree\"\n",
    "    if n == 3.0: return \"unrelated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced')\n",
    "x_train = np.hstack((train_headline_vectors, train_body_vectors, np.expand_dims(train_cosine, axis=1)))\n",
    "y_train = train.apply(lambda row: label(row['Stance']), axis=1)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.hstack([test_headline_vectors, test_body_vectors, np.expand_dims(test_cosine, axis=1)])\n",
    "y_test = test.apply(lambda row: label(row['Stance']), axis=1)\n",
    "y_prediction = model.predict(x_test)\n",
    "accuracy_score(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = list(test['Stance'])\n",
    "predicted = list(map(lambda x: unlabel(x), y_prediction))\n",
    "s.report_score(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
