{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>wmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.649580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.160659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.322309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.895938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>3.598486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate       wmd  \n",
       "0  What is the step by step guide to invest in sh...             0  0.649580  \n",
       "1  What would happen if the Indian government sto...             0  2.160659  \n",
       "2  How can Internet speed be increased by hacking...             0  2.322309  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  5.895938  \n",
       "4            Which fish would survive in salt water?             0  3.598486  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/quora-train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['question1', 'question2']]\n",
    "y = df['is_duplicate']\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_indexes, test_indexes = next(sss.split(x, y))\n",
    "train = df.iloc[train_indexes]\n",
    "test = df.iloc[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FastTextKeyedVectors.load_word2vec_format('model/fasttext/quora.vec')\n",
    "all_words = set(model.vocab.keys())\n",
    "int_vocab = {word:i for i,word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_word = \"与己方便\" # strange word from vocab\n",
    "def extract_sequence(question):\n",
    "    tokens = list(filter(lambda word: word in all_words, word_tokenize(question.lower())))\n",
    "    if(len(tokens) < MAX_SEQUENCE_LENGTH):\n",
    "        tokens = tokens + [pad_word for i in range(MAX_SEQUENCE_LENGTH-len(tokens))]\n",
    "    return [int_vocab[word] for word in tokens]\n",
    "\n",
    "def extract_sequence_column(questions):\n",
    "    result = []\n",
    "    for q in questions:\n",
    "        result.append(extract_sequence(q))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_sequences = np.array(extract_sequence_column(train['question1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2_sequences = np.array(extract_sequence_column(train['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_q1_sequences = np.array(extract_sequence_column(test['question1']))\n",
    "test_q2_sequences = np.array(extract_sequence_column(test['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.dump(train_q1_sequences, open('data/train_q1_sequences.pickle', 'wb'))\n",
    "pc.dump(train_q2_sequences, open('data/train_q2_sequences.pickle', 'wb'))\n",
    "pc.dump(test_q1_sequences, open('data/test_q1_sequences.pickle', 'wb'))\n",
    "pc.dump(test_q2_sequences, open('data/test_q2_sequences.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = model.get_embedding_layer()\n",
    "lstm_layer = Bidirectional(LSTM(300,dropout=0.332,recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "x2 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "wmd_input = Input(shape=(1, ))\n",
    "\n",
    "merged = concatenate([x1, x2, wmd_input])\n",
    "merged = Dropout(0.4)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(130, activation='relu')(merged)\n",
    "merged = Dropout(0.075)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, wmd_input],outputs=output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = 'model/fold-checkpoint.h5'\n",
    "\n",
    "X_train_q1 = np.vstack([train_q1_sequences, train_q2_sequences])\n",
    "X_train_q2 = np.vstack([train_q2_sequences, train_q1_sequences])\n",
    "X_train_wmd = np.concatenate([np.array(train['wmd']), np.array(train['wmd'])])\n",
    "\n",
    "X_val_q1 = np.vstack([test_q1_sequences, test_q2_sequences])\n",
    "X_val_q2 = np.vstack([test_q2_sequences, test_q1_sequences])\n",
    "X_test_wmd = np.concatenate([np.array(test['wmd']), np.array(test['wmd'])])\n",
    "\n",
    "y_train = np.concatenate([train['is_duplicate'], train['is_duplicate']])\n",
    "y_val = np.concatenate([test['is_duplicate'], test['is_duplicate']])\n",
    "\n",
    "# Train.\n",
    "model.fit([X_train_q1, X_train_q2, X_train_wmd], y_train,\n",
    "          validation_data=([X_val_q1, X_val_q2, X_test_wmd], y_val),\n",
    "        batch_size=128,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        verbose=1,\n",
    "\n",
    "        callbacks=[\n",
    "        # Stop training when the validation loss stops improving.\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.001,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "        ),\n",
    "        # Save the weights of the best epoch.\n",
    "        ModelCheckpoint(\n",
    "            model_checkpoint_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=2,\n",
    "        ),\n",
    "        ],\n",
    ")\n",
    "\n",
    "# Restore the best epoch.\n",
    "model.load_weights(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
